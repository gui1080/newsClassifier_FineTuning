# https://huggingface.co/google-bert/bert-base-multilingual-uncased

# --------------------------------------------------------

from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from transformers import DataCollatorWithPadding
import evaluate
import pandas as pd
from datasets import Dataset
from enelvo.normaliser import Normaliser
from transformers import Trainer
import logging
from transformers import Trainer, TrainingArguments, TrainerCallback  # Ensure TrainerCallback is imported
from transformers import TrainingArguments

from make_embedding import set_bert, set_bart, set_roberta

# Normalizador de texto
# --------------------------------------------------------

normalizador = Normaliser(tokenizer='readable', capitalize_inis=True, 
                          capitalize_pns=True, capitalize_acs=True, 
                          sanitize=True)

def normalize_text(input_text):

    #final_text = normalizador.normalise(text)
    final_text = " ".join(word.strip() for word in input_text.split())
    final_text = re.sub(r'(?<!\*)\*\*(?!\*)', '', final_text)
    return final_text


# --------------------------------------------------------

etl = True

if etl:

    print("Loading csv")

    # Load the CSV file
    df_gpt4 = pd.read_csv("../DATASETS_NOTICIAS/GPT4o/noticias_gpt4_n2000.csv")

    df_original = pd.read_csv("../DATASETS_NOTICIAS/G1_Metropoles_MinhaExtracaoFinal/dataset_noticias_final.csv")
    df_original = df_original.sample(n=2000, replace=False)

    print("Loaded")

    print("Labels made")

    df_gpt4['text'] = df_gpt4['text'].apply(normalize_text)
    print("df_gpt4 normalizado")

    df_original['text'] = df_original['text'].apply(normalize_text)
    print("df_original normalizado")

    # LABELS
    # AI MADE = LABEL 1
    df_gpt4['label'] = 1

    # HUMAN MADE = LABEL 0
    df_original['label'] = 0

    # Concatenate both datasets
    df = pd.concat([df_gpt4, df_original], ignore_index=True)

    print("concat complete")

    df.to_csv("df_sklearn_gpt40_original_BERT.csv", index=False)

    print("df_sklearn_gpt40_original_BERT.csv saved!")

    # Convert to a Hugging Face Dataset
    dataset = Dataset.from_pandas(df)

    print("limpeza feita")

else:

    df_gpt4 = pd.read_csv("df_fine_tune_gpt40_original.csv")
    dataset = Dataset.from_pandas(df)

# --------------------------------------------------------

# Split the data
train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)

# Convert splits to Hugging Face Datasets
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

print("Split dados")

# Load the tokenizer
model_checkpoint = "bert-base-multilingual-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

# Tokenize the datasets
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

print("Tokenize dados")

train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# Remove unnecessary columns and set the format
train_dataset = train_dataset.remove_columns(["text"])
test_dataset = test_dataset.remove_columns(["text"])
train_dataset.set_format("torch")
test_dataset.set_format("torch")

print("Map dados")

print(type(train_dataset[1]))

'''
-> <class 'dict'>

print(type(train_dataset[1])) -> print(train_dataset[1])

{'label': tensor(1), '__index_level_0__': tensor(1436), 'input_ids': tensor([   101,    115,    115,  15727,  14771,  41225,  10147,  32249, 110134,
         10183,  97306,  10121,  12211,  89270,  15906,    173,  77005,  11033,
         21593,  10104,  62973,    115,    115,  11289,  10437,  44671,  92795,
         10922,  10121,  68727,  10138,  16304,  24495,  11043,    117,    183,
         11419,    118,  20329,  10242,  20304,  10330,  12292,  98227,  10149,
         12264,    117,  32249, 110134,    117,  72707,  10266,  10437,  39177,
         10121,  12921,    183,  12211,  89270,  15906,  15696,    183,  77005,
         11033,  98730,  10147,  10493,  63586,  99793,  10104,  62973,    119,
           138,  38439,  10922,  11284,  11244,  30656,  11639,  29715,  19960,
         92883,  10403,  10104,  62414,    173,  68051,  10107,  10266, 108051,
         14343,    117,  10121,  10126,  87438,  70911,  11008,  10266,  48056,
         78411,    169,  57113,    119, 110134,  21749,  15087,  92795,  12292,
         11047,  10293,  20720,  91190,  10183,  10293,  12282,  91905,    117,
         12058,  82310,  10138,  10121,  10146, 107522,  10107,  11339,  10486,
         41836,  10149,  15783,  12067,  10212,  96503,  42182,  10147,  24423,
         10212,  10427,  99793,  47981,  10107,    119,  19311,  12637,    117,
         24344,  30776,  40300,  64004,  10437, 105348,  16046,  36818,  10402,
         83959,  44122,  10107,  31536,    173,    169,  62973,  26777,  65075,
         10129,  67710,    119,  15727,  14771,  10225,    169,  13853,  10113,
         11066,  23086,    156,    119,  52807,  10143,  18637,  14492,  10149,
         12109,  10104,  14822,    113,    158,  11565,  11273,  15417,    114,
         13395,  40625,  10107,  10266,  41225,  10129,    169,  38439,  10922,
           119,    107,    152,  12211,  89270,  15906,    173,    183,  77005,
         11033,  12372,  82964,  10611,  11639,  93966,  10107,  10225,  24807,
         10107,  37882,  84232,  10107,  10104,  85862,  91863,    107,    117,
         15718,  52807,    119,    107,  67915,  11639,  10381,  36111,  11008,
         10146,  57113,  10107,  47981,  10107,    173, 105677,    117,  60041,
         17560,  10437,  11041,  31991,  10104,  14352,  13689,  10121,    263,
         91430,  10303,  26986,  13851, 101221,  10107,  10143,  18691,  16114,
           173,  10143,  12820, 107795,  12142,    119,    107,  76352,  68051,
           117,    183,  13853,  13832,    149,    119,  13902,  10143,  18637,
         86646,  47867,    113,  26578,  25054,  11127,    114,    117,  26043,
         46913,  10138,    131,    107,    234,  12229,  25125,  49480,  10216,
         16719,  39540,  47992,  30045,  49917,  10317,  10139,  10245,  44010,
         10922,  10690,  21974,  14584,  96483,  48949,  15828,    119,    152,
         29632,  17478,  19847,  10493, 110829,  31910,  10317,  10132,  58695,
         36818,    173,  11420,  10266,  65789,  10107,  27920,  11813,  42112,
           119,    107,    138,  13722,  13643,  15241,  10113,  11420,  10126,
         54877,  10610,  62110,  17478,    132,  12593,  11961,  90226,  10132,
         68821,  36818,  10192,  12264,    119, 108127,  23037,  31001,  11234,
         11372,  16294,  10147,  10220,  10427,  84697,  10107,  10104,  92304,
         62932,  17370,  12292,  93317,  38061,  12947,  71695,  14584,  10690,
        107522,  10107,  31536,    117,  18377,  11971,  34338,  21584,  27647,
         10129,  15554,  29715,  10614,  71030,    119,    138,  92795,  10922,
         10104, 110134,  54328,  10266,  10293,  45189,  10614,  10392,  46128,
         10104,  45844,  27187,  14343,  10192,  12264,    117,  12058,  88806,
         10690,  19879,  33387,  12477,  57021,  52693,  10220,  78198,  34762,
         10129,  39327,  34914,  38938, 108898,  26777,  65075,  10129,  67710,
         10107,    119,    138,  11639,  14356,  10398,  62414,  31422,    169,
         68487,  10149,  29956,  28025,  13621,  11488,  24495,  11911,  10132,
         68872,  10398,  28719,  73733,  10107,    173,  10603,  43373,  10477,
         32281,  10220,  71258,  10427,  15928,  64116,  10107,  87605,  10291,
           119,    225,  26080,  10121,  21553,  71695,  11449,  10170,  45090,
         12947,  46424,  55167,    173,  11573,  25598,  10107,  90481,  10104,
         79532,    117,  49772,  56956,  10121,  10146,  42246,  12292,  10690,
         12211,  89270,  15906,    173,  77005,  11033,  36055,  11449,    169,
         10493,  10293,  13493,  13722,  66005,  10192,  10794,    102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1])}
'''